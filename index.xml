<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hicham&#39; Blog</title>
    <link>https://helboukkouri.github.io/</link>
    <description>Recent content on Hicham&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 09 Nov 2020 20:59:15 +0100</lastBuildDate><atom:link href="https://helboukkouri.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</title>
      <link>https://helboukkouri.github.io/posts/character-bert/</link>
      <pubDate>Mon, 09 Nov 2020 20:59:15 +0100</pubDate>
      
      <guid>https://helboukkouri.github.io/posts/character-bert/</guid>
      <description>Difference between BERT and CharacterBERT CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.</description>
    </item>
    
  </channel>
</rss>
