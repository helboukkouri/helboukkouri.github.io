<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters | Hicham EL BOUKKOURI</title>
<meta name=keywords content><meta name=description content="
What is CharacterBERT and how is it different from BERT ?
CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT."><meta name=author content="Hicham EL BOUKKOURI"><link rel=canonical href=https://helboukkouri.github.io/posts/character-bert/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://helboukkouri.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://helboukkouri.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://helboukkouri.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://helboukkouri.github.io/apple-touch-icon.png><link rel=mask-icon href=https://helboukkouri.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://helboukkouri.github.io/posts/character-bert/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://helboukkouri.github.io/posts/character-bert/"><meta property="og:site_name" content="Hicham EL BOUKKOURI"><meta property="og:title" content="CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"><meta property="og:description" content=" What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-11-09T20:59:15+01:00"><meta property="article:modified_time" content="2020-11-09T20:59:15+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"><meta name=twitter:description content="
What is CharacterBERT and how is it different from BERT ?
CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://helboukkouri.github.io/posts/"},{"@type":"ListItem","position":2,"name":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","item":"https://helboukkouri.github.io/posts/character-bert/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","name":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","description":" What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].\nThe next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.\n","keywords":[],"articleBody":" What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].\nThe next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.\nContext-independent token representations in BERT vs. in CharacterBERT (Source: [2])\nLet’s imagine that the word “Apple” is an unknown word (i.e. it does not appear in BERT’s WordPiece vocabulary), then BERT splits it into known WordPieces: [Ap, ##ple], where ## are used to designate WordPieces that are not at the beginning of a word. Then, each subword unit is embedded using a WordPiece embedding matrix, producing two output vectors.\nOn the other hand, CharacterBERT does not have a WordPiece vocabulary and can handle any input token as long as it is not unreasonably long (i.e. under 50 characters). Instead of splitting “Apple”, CharacterBERT reads it as a sequence of characters: [A, p, p, l, e]. Each character is then represented using a character embedding matrix, producing a sequence of character embeddings. This sequence is then fed to multiple CNNs, each responsible for scanning the sequence n-characters at a time, with n =[1..7]. All CNN outputs are aggregated into a single vector that is then projected down to the desired dimension using Highway Layers [3]. This final projection is the context-independent representation of the word “Apple”, which will be combined with position and segment embeddings before being fed to multiple Transformer Layers as in BERT.\nWhy CharacterBERT instead of BERT? CharacterBERT acts almost as a drop in replacement for BERT that\nProduces a single embedding for any input token Does not rely on a WordPiece vocabulary The first point is clearly desirable as working with single embeddings is far more convenient than having a variable number of WordPiece vectors for each token. As for the second point, it is particularly relevant when working in specialized domains (e.g. medical domain, legal domain, …). In fact, the common practice when building specialized versions of BERT (e.g. BioBERT [4], BlueBERT [5] and some SciBERT [6] models) is to re-train the original model on a set of specialized texts. As a result, most SOTA specialized models keep the original general-domain WordPiece vocabulary which is not suited for specialized domain applications.\nThe table below shows the difference between the original general-domain vocabulary and a medical WordPiece vocabulary that was built on medical corpora: MIMIC [7] and PMC OA [8].\nTokenization of domain-specific terms using wordpiece vocabularies of different domains (Source: [2])\nWe can clearly see that BERT’s vocabulary is not very well suited for specialized terms (e.g. “choledocholithiasis” is split into [cho, led, och, oli, thi, asi, s]). The medical wordpiece works better however it has its limits as well (e.g. “borborygmi” into [bor, bor, yg, mi]). Therefore, in order to avoid any biases that may come from using the wrong WordPiece vocabulary, and in an effort to got back to conceptually simpler models, a variant of BERT was proposed: CharacterBERT.\nHow is CharacterBERT tested against BERT? BERT and CharacterBERT are compared in a classic scenario where a general model is first pre-trained before serving as an initialisation for the pre-training of a specialized version.\nNote: we focus here on the English language and the medical domain.\nPre-training corpora (Source: [2])\nTo be as fair as possible, both BERT and CharacterBERT are pre-trained from scratch in exactly the same conditions. Then, each pre-trained model is evaluated on multiple medical tasks. Let’s take an example.\nEvaluation tasks (Source: [2])\ni2b2/VA 2010 [9] is a competition that consists in multiple tasks including the clinical concept detection task which was used to evaluate our models. The goal is to detect three types of clinical concepts: Problem, Treatment and Test. An example is given in the far left section of the figure above.\nAs usual, we evaluate our models by first training on the training set. At each iteration, the model is tested on a separate validation set, allowing us to save the best iteration. Finally, after going through all the iterations, a score (here a strict F1 score) is computed on the test set using the model from the best iteration. This whole procedure is then repeated 9 more times using different random seeds, which allows us to account for some of the variance and report final model performances as: mean ± std.\nNote: more details are available in the paper [2]\nWhat are the results? Evaluation results (Source: [2])\nIn most cases, CharacterBERT outperformed its BERT counterpart.\nNote: The only exception is the ClinicalSTS task where the medical CharacterBERT got (on average) a lower score than the BERT version. This may be due to the task dataset being small (1000 examples vs. 30,000 on average for other tasks) and should be investigated.\nBonus: Robustness to Noise Besides pure performance, another interesting aspect is whether the models are robust to noisy inputs. In fact, we evaluated BERT and CharacterBERT on noisy versions of the MedNLI task [10] where (put simply) the goal is to say whether two medical sentences are in contradiction with each other. Here, a noise level of X% means that each character in the text if either replaced or swapped with a X% probability. The results are displayed on the figure below.\nBERT and CharacterBERT fine-tuned on noisy (misspelled) versions of MEDNLI (Source: [2])\nAs you can see, the medical CharacterBERT model seems to be more robust than medical BERT: the initial gap between the two models of ~1% accuracy grows to ~3% when adding noise to all splits, and ~5% when surprising the models with noise only in the test set.\nWhat about the downsides of CharacterBERT? The main downside of CharacterBERT is its slower pre-training speed. This is due to:\nthe CharacterCNN module that is slower to train; but mainly because the model is working at the token-level:\nit updates a large token vocabulary at each pre-training iteration. Note: However, CharacterBERT is just as fast as BERT during inference (actually, it is even a bit faster) and pre-trained models are available so you can skip the pre-training step altogether 😊!\nConclusion All in all, CharacterBERT is a simple variant of BERT that replaces the WordPiece system with a CharacterCNN (just like ELMo before that). Evaluation results on multiple medical tasks show that this change is beneficial: improved performance \u0026 improved robustness to misspellings. Hopefully, this model will motivate more research towards word-level open-vocabulary transformer-based language models (e.g. applying the same idea to ALBERT [11], ERNIE [12]).\nOriginal paper:\nhttps://arxiv.org/abs/2010.10392\nCode \u0026 pre-trained models: https://github.com/helboukkouri/character-bert\nWork done by:\nHicham El Boukkouri (myself), Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum and Junichi Tsujii\nReferences [1] Peters, Matthew E., et al. “Deep contextualized word representations.” arXiv preprint arXiv:1802.05365 (2018). [2] El Boukkouri, Hicham, et al. “CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters.” arXiv preprint arXiv:2010.10392 (2020). [3] Srivastava, Rupesh Kumar, Klaus Greff, and Jürgen Schmidhuber. “Highway networks.” arXiv preprint arXiv:1505.00387 (2015). [4] Lee, Jinhyuk, et al. “BioBERT: a pre-trained biomedical language representation model for biomedical text mining.” Bioinformatics 36.4 (2020): 1234–1240. [5] Peng, Yifan, Shankai Yan, and Zhiyong Lu. “Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets.” arXiv preprint arXiv:1906.05474 (2019). [6] Beltagy, Iz, Kyle Lo, and Arman Cohan. “SciBERT: A pretrained language model for scientific text.” arXiv preprint arXiv:1903.10676 (2019). [7] Johnson, Alistair, et al. “MIMIC-III Clinical Database” (version 1.4). PhysioNet (2016), https://doi.org/10.13026/C2XW26. [8] PMC OA corpus: https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ [9] Uzuner, Özlem, et al. “2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text.” Journal of the American Medical Informatics Association 18.5 (2011): 552–556. [10] Shivade, Chaitanya. “MedNLI - A Natural Language Inference Dataset For The Clinical Domain” (version 1.0.0). PhysioNet (2019), https://doi.org/10.13026/C2RS98. [11] Lan, Zhenzhong, et al. “ALBERT: A lite BERT for self-supervised learning of language representations.” arXiv preprint arXiv:1909.11942 (2019). [12] Sun, Yu, et al. “ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding.” AAAI. 2020. ","wordCount":"1370","inLanguage":"en","datePublished":"2020-11-09T20:59:15+01:00","dateModified":"2020-11-09T20:59:15+01:00","author":{"@type":"Person","name":"Hicham EL BOUKKOURI"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://helboukkouri.github.io/posts/character-bert/"},"publisher":{"@type":"Organization","name":"Hicham EL BOUKKOURI","logo":{"@type":"ImageObject","url":"https://helboukkouri.github.io/images/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://helboukkouri.github.io/ accesskey=h title="Home (Alt + H)">Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://helboukkouri.github.io/posts/ title="Blog Posts"><span>Blog Posts</span></a></li><li><a href=https://helboukkouri.github.io/games/ title="Game Development"><span>Game Development</span></a></li><li><a href=https://helboukkouri.github.io/embedding-visualization/ title="Embedding Universe"><span>Embedding Universe</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</h1><div class=post-meta><span title='2020-11-09 20:59:15 +0100 CET'>November 9, 2020</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Hicham EL BOUKKOURI</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#what-is-characterbert-and-how-is-it-different-from-bert aria-label="What is CharacterBERT and how is it different from BERT ?">What is CharacterBERT and how is it different from BERT ?</a></li><li><a href=#why-characterbert-instead-of-bert aria-label="Why CharacterBERT instead of BERT?">Why CharacterBERT instead of BERT?</a></li><li><a href=#how-is-characterbert-tested-against-bert aria-label="How is CharacterBERT tested against BERT?">How is CharacterBERT tested against BERT?</a></li><li><a href=#what-are-theresults aria-label="What are the results?">What are the results?</a></li><li><a href=#bonus-robustness-tonoise aria-label="Bonus: Robustness to Noise">Bonus: Robustness to Noise</a></li><li><a href=#what-about-the-downsides-of-characterbert aria-label="What about the downsides of CharacterBERT?">What about the downsides of CharacterBERT?</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><hr><h2 id=what-is-characterbert-and-how-is-it-different-from-bert>What is CharacterBERT and how is it different from BERT ?<a hidden class=anchor aria-hidden=true href=#what-is-characterbert-and-how-is-it-different-from-bert>#</a></h2><p>CharacterBERT is a variant of BERT that tries to go back to the simpler days where <strong>models produced single embeddings for single words</strong> (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a <strong>CharacterCNN module just like the one that was used in ELMo</strong> <em><a href=#reference.1>[1]</a></em>.</p><p>The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.</p><p><img alt="alt text" loading=lazy src=/images/character-bert/characterbert.png title="BERT vs. CharacterBERT"></p><center><small>Context-independent token representations in BERT vs. in CharacterBERT (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>Let&rsquo;s imagine that the word &ldquo;<strong>Apple</strong>&rdquo; is an unknown word (i.e. it does not appear in BERT&rsquo;s WordPiece vocabulary), then BERT <strong>splits it into known WordPieces</strong>: [<strong>Ap</strong>, <strong>##ple</strong>], where <strong>##</strong> are used to designate WordPieces that are not at the beginning of a word. Then, each subword unit is <strong>embedded using a WordPiece embedding matrix</strong>, producing <strong>two output vectors</strong>.</p><p>On the other hand, <strong>CharacterBERT does not have a WordPiece vocabulary and can handle any input token as long</strong> as it is not unreasonably long (i.e. under 50 characters). Instead of splitting &ldquo;<strong>Apple</strong>&rdquo;, CharacterBERT reads it as a <strong>sequence of characters</strong>: [<strong>A</strong>, <strong>p</strong>, <strong>p</strong>, <strong>l</strong>, <strong>e</strong>]. Each character is then <strong>represented using a character embedding matrix</strong>, producing a sequence of <strong>character embeddings</strong>. This sequence is then <strong>fed to multiple CNNs</strong>, each responsible for scanning the sequence <strong>n-characters at a time</strong>, with n =[1..7]. All CNN outputs are <strong>aggregated into a single vector</strong> that is then <strong>projected down</strong> to the desired dimension using Highway Layers <em><a href=#reference.3>[3]</a></em>. This final projection is the <strong>context-independent representation</strong> of the word &ldquo;Apple&rdquo;, which will be combined with position and segment embeddings before being fed to multiple Transformer Layers as in BERT.</p><h2 id=why-characterbert-instead-of-bert>Why CharacterBERT instead of BERT?<a hidden class=anchor aria-hidden=true href=#why-characterbert-instead-of-bert>#</a></h2><p>CharacterBERT acts almost as a <strong>drop in replacement for BERT</strong> that</p><ul><li>Produces a single embedding for any input token</li><li>Does not rely on a WordPiece vocabulary</li></ul><p>The first point is clearly desirable as working with single embeddings is <strong>far more convenient</strong> than having a variable number of WordPiece vectors for each token. As for the second point, it is particularly relevant when working in <strong>specialized domains</strong> (e.g. medical domain, legal domain, …). In fact, the common practice when building <strong>specialized versions of BERT</strong> (e.g. BioBERT <em><a href=#reference.4>[4]</a></em>, BlueBERT <em><a href=#reference.5>[5]</a></em> and some SciBERT <em><a href=#reference.6>[6]</a></em> models) is to <strong>re-train</strong> the original model on a set of specialized texts. As a result, most SOTA specialized models <strong>keep the original general-domain WordPiece vocabulary</strong> which is not suited for specialized domain applications.</p><p>The table below shows the difference between the <strong>original general-domain vocabulary</strong> and a <strong>medical WordPiece vocabulary</strong> that was built on medical corpora: MIMIC <em><a href=#reference.7>[7]</a></em> and PMC OA <em><a href=#reference.8>[8]</a></em>.</p><p><img alt="alt text" loading=lazy src=/images/character-bert/medical-terms.png title="Medical term WordPiece tokenization"></p><center><small>Tokenization of domain-specific terms using wordpiece vocabularies of different domains (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>We can clearly see that BERT&rsquo;s vocabulary is not very well suited for specialized terms (e.g. &ldquo;choledocholithiasis&rdquo; is split into [cho, led, och, oli, thi, asi, s]). The medical wordpiece works better however it has its limits as well (e.g. &ldquo;borborygmi&rdquo; into [bor, bor, yg, mi]). Therefore, in order to <strong>avoid any biases</strong> that may come from using the wrong WordPiece vocabulary, and in an effort to got back to <strong>conceptually simpler models</strong>, a variant of BERT was proposed: CharacterBERT.</p><h2 id=how-is-characterbert-tested-against-bert>How is CharacterBERT tested against BERT?<a hidden class=anchor aria-hidden=true href=#how-is-characterbert-tested-against-bert>#</a></h2><p>BERT and CharacterBERT are compared in a classic scenario where a general model is first pre-trained before serving as an initialisation for the pre-training of a specialized version.</p><blockquote><p><strong>Note</strong>: <em>we focus here on the English language and the medical domain.</em></p></blockquote><p><img alt="alt text" loading=lazy src=/images/character-bert/pretraining-corpora.png title="Pre-training corpora"></p><center><small>Pre-training corpora (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>To be <strong>as fair as possible</strong>, both BERT and CharacterBERT are pre-trained from scratch in <strong>exactly the same conditions</strong>. Then, each pre-trained model is evaluated on multiple medical tasks. Let&rsquo;s take an example.</p><p><img alt="alt text" loading=lazy src=/images/character-bert/evaluation-tasks.png title="Evaluation tasks"></p><center><small>Evaluation tasks (Source: <a href=#reference.2>[2]</a>)</small></center><br><p><strong>i2b2/VA 2010</strong> [9] is a competition that consists in multiple tasks including the <strong>clinical concept detection task</strong> which was used to evaluate our models. The goal is to detect three types of clinical concepts: <strong>Problem</strong>, <strong>Treatment</strong> and <strong>Test</strong>. An example is given in the far left section of the figure above.</p><p>As usual, we evaluate our models by first training on the training set. At each iteration, the model is tested on a separate validation set, allowing us to save the best iteration. Finally, after going through all the iterations, a score (here a strict F1 score) is computed on the test set using the model from the best iteration. This whole procedure is then repeated 9 more times using different random seeds, which allows us to account for some of the variance and report final model performances as: <strong>mean ± std</strong>.</p><blockquote><p><strong>Note</strong>: <em>more details are available in the paper <a href=#reference.2>[2]</a></em></p></blockquote><h2 id=what-are-theresults>What are the results?<a hidden class=anchor aria-hidden=true href=#what-are-theresults>#</a></h2><p><img alt="alt text" loading=lazy src=/images/character-bert/evaluation-results.png title="Evaluation results"></p><center><small>Evaluation results (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>In most cases, <strong>CharacterBERT outperformed its BERT counterpart</strong>.</p><blockquote><p><strong>Note</strong>: <em>The only exception is the ClinicalSTS task where the medical CharacterBERT got (on average) a lower score than the BERT version. This may be due to the task dataset being small (1000 examples vs. 30,000 on average for other tasks) and should be investigated.</em></p></blockquote><h2 id=bonus-robustness-tonoise>Bonus: Robustness to Noise<a hidden class=anchor aria-hidden=true href=#bonus-robustness-tonoise>#</a></h2><p>Besides pure performance, another interesting aspect is <strong>whether the models are robust to noisy inputs</strong>. In fact, we evaluated BERT and CharacterBERT on noisy versions of the MedNLI task <em><a href=#reference.10>[10]</a></em> where (put simply) the goal is to say whether two medical sentences are in contradiction with each other. Here, a noise level of X% means that each character in the text if either replaced or swapped with a X% probability. The results are displayed on the figure below.</p><p><img alt="alt text" loading=lazy src=/images/character-bert/noise-robustness.png title="Robustness to noise"></p><center><small>BERT and CharacterBERT fine-tuned on noisy (misspelled) versions of MEDNLI (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>As you can see, the medical CharacterBERT model seems to be <strong>more robust than medical BERT</strong>: the initial gap between the two models of ~1% accuracy grows to ~<strong>3%</strong> when adding noise to all splits, and ~<strong>5%</strong> when surprising the models with noise only in the test set.</p><h2 id=what-about-the-downsides-of-characterbert>What about the downsides of CharacterBERT?<a hidden class=anchor aria-hidden=true href=#what-about-the-downsides-of-characterbert>#</a></h2><p>The main downside of CharacterBERT is its <strong>slower pre-training speed</strong>. This is due to:</p><ul><li>the CharacterCNN module that is <strong>slower to train</strong>;</li><li>but mainly because the model is working at the <strong>token-level</strong>:<br>it updates a <strong>large token vocabulary</strong> at each pre-training iteration.</li></ul><blockquote><p><strong>Note</strong>: <em>However, CharacterBERT is just as fast as BERT <strong>during inference</strong> (actually, it is even a bit faster) and <strong>pre-trained models are available</strong> so you can skip the pre-training step altogether 😊!</em></p></blockquote><hr><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>All in all, CharacterBERT is a <strong>simple variant of BERT</strong> that replaces the WordPiece system with a CharacterCNN (just like ELMo before that). Evaluation results on multiple medical tasks show that this change is beneficial: <strong>improved performance</strong> & <strong>improved robustness to misspellings</strong>. Hopefully, this model will motivate more research towards word-level open-vocabulary transformer-based language models (e.g. applying the same idea to ALBERT <em><a href=#reference.11>[11]</a></em>, ERNIE <em><a href=#reference.12>[12]</a></em>).</p><blockquote><p><strong>Original paper</strong>:<br><a href=https://arxiv.org/abs/2010.10392>https://arxiv.org/abs/2010.10392</a></p></blockquote><blockquote><p><strong>Code & pre-trained models</strong>:<br><a href=https://github.com/helboukkouri/character-bert>https://github.com/helboukkouri/character-bert</a></p></blockquote><blockquote><p><strong>Work done by</strong>:<br>Hicham El Boukkouri (myself), <a href="https://scholar.google.com/citations?user=-mCQhtIAAAAJ&amp;hl=en">Olivier Ferret</a>, <a href="https://scholar.google.fr/citations?user=l7XLFhEAAAAJ&amp;hl=en">Thomas Lavergne</a>, <a href="https://scholar.google.com/citations?user=OODRveoAAAAJ&amp;hl=en">Hiroshi Noji</a>, <a href="https://scholar.google.fr/citations?user=0LjUNAsAAAAJ&amp;hl=en">Pierre Zweigenbaum</a> and <a href="https://scholar.google.ca/citations?user=h3aNnAIAAAAJ&amp;hl=en">Junichi Tsujii</a></p></blockquote><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a id=reference.1></a>[1] Peters, Matthew E., et al. &ldquo;Deep contextualized word representations.&rdquo; arXiv preprint arXiv:1802.05365 (2018).</li><li><a id=reference.2></a>[2] El Boukkouri, Hicham, et al. &ldquo;CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters.&rdquo; arXiv preprint arXiv:2010.10392 (2020).</li><li><a id=reference.3></a>[3] Srivastava, Rupesh Kumar, Klaus Greff, and Jürgen Schmidhuber. &ldquo;Highway networks.&rdquo; arXiv preprint arXiv:1505.00387 (2015).</li><li><a id=reference.4></a>[4] Lee, Jinhyuk, et al. &ldquo;BioBERT: a pre-trained biomedical language representation model for biomedical text mining.&rdquo; Bioinformatics 36.4 (2020): 1234–1240.</li><li><a id=reference.5></a>[5] Peng, Yifan, Shankai Yan, and Zhiyong Lu. &ldquo;Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets.&rdquo; arXiv preprint arXiv:1906.05474 (2019).</li><li><a id=reference.6></a>[6] Beltagy, Iz, Kyle Lo, and Arman Cohan. &ldquo;SciBERT: A pretrained language model for scientific text.&rdquo; arXiv preprint arXiv:1903.10676 (2019).</li><li><a id=reference.7></a>[7] Johnson, Alistair, et al. &ldquo;MIMIC-III Clinical Database&rdquo; (version 1.4). PhysioNet (2016), <a href=https://doi.org/10.13026/C2XW26>https://doi.org/10.13026/C2XW26</a>.</li><li><a id=reference.8></a>[8] PMC OA corpus: <a href=https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/>https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/</a></li><li><a id=reference.9></a>[9] Uzuner, Özlem, et al. &ldquo;2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text.&rdquo; Journal of the American Medical Informatics Association 18.5 (2011): 552–556.</li><li><a id=reference.10></a>[10] Shivade, Chaitanya. &ldquo;MedNLI - A Natural Language Inference Dataset For The Clinical Domain&rdquo; (version 1.0.0). PhysioNet (2019), <a href=https://doi.org/10.13026/C2RS98>https://doi.org/10.13026/C2RS98</a>.</li><li><a id=reference.11></a>[11] Lan, Zhenzhong, et al. &ldquo;ALBERT: A lite BERT for self-supervised learning of language representations.&rdquo; arXiv preprint arXiv:1909.11942 (2019).</li><li><a id=reference.12></a>[12] Sun, Yu, et al. &ldquo;ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding.&rdquo; AAAI. 2020.</li></ul><hr></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on x" href="https://x.com/intent/tweet/?text=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&amp;url=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f&amp;title=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&amp;summary=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&amp;source=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f&title=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on whatsapp" href="https://api.whatsapp.com/send?text=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters%20-%20https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on telegram" href="https://telegram.me/share/url?text=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&amp;url=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on ycombinator" href="https://news.ycombinator.com/submitlink?t=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&u=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://helboukkouri.github.io/>Hicham EL BOUKKOURI</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>