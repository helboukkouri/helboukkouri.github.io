<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters - Hicham' Blog</title><meta name=keywords content><meta name=description content="What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT."><meta name=author content="Hicham EL BOUKKOURI"><link rel=canonical href=https://helboukkouri.github.io/posts/character-bert/><link href=https://helboukkouri.github.io/assets/css/stylesheet.min.dc2a1df0cfcb4402967907d442813bfe2a60fce77618707d09a2aab64fdf5adb.css integrity="sha256-3Cod8M/LRAKWeQfUQoE7/ipg/Od2GHB9CaKqtk/fWts=" rel="preload stylesheet" as=style><link rel=manifest href=https://helboukkouri.github.io/site.webmanifest><link rel=icon href=https://helboukkouri.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://helboukkouri.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://helboukkouri.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://helboukkouri.github.io/apple-touch-icon.png><link rel=mask-icon href=https://helboukkouri.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.76.5"><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:title" content="CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"><meta property="og:description" content="What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT."><meta property="og:type" content="article"><meta property="og:url" content="https://helboukkouri.github.io/posts/character-bert/"><meta property="article:published_time" content="2020-11-09T20:59:15+01:00"><meta property="article:modified_time" content="2020-11-09T20:59:15+01:00"><meta property="og:site_name" content="Hicham' Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"><meta name=twitter:description content="What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","name":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","description":"What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for …","keywords":[],"articleBody":" What is CharacterBERT and how is it different from BERT ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].\nThe next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.\nContext-independent token representations in BERT vs. in CharacterBERT (Source: [2])\nLet’s imagine that the word “Apple” is an unknown word (i.e. it does not appear in BERT’s WordPiece vocabulary), then BERT splits it into known WordPieces: [Ap, ##ple], where ## are used to designate WordPieces that are not at the beginning of a word. Then, each subword unit is embedded using a WordPiece embedding matrix, producing two output vectors.\nOn the other hand, CharacterBERT does not have a WordPiece vocabulary and can handle any input token as long as it is not unreasonably long (i.e. under 50 characters). Instead of splitting “Apple”, CharacterBERT reads it as a sequence of characters: [A, p, p, l, e]. Each character is then represented using a character embedding matrix, producing a sequence of character embeddings. This sequence is then fed to multiple CNNs, each responsible for scanning the sequence n-characters at a time, with n =[1..7]. All CNN outputs are aggregated into a single vector that is then projected down to the desired dimension using Highway Layers [3]. This final projection is the context-independent representation of the word “Apple”, which will be combined with position and segment embeddings before being fed to multiple Transformer Layers as in BERT.\nWhy CharacterBERT instead of BERT? CharacterBERT acts almost as a drop in replacement for BERT that\n Produces a single embedding for any input token Does not rely on a WordPiece vocabulary  The first point is clearly desirable as working with single embeddings is far more convenient than having a variable number of WordPiece vectors for each token. As for the second point, it is particularly relevant when working in specialized domains (e.g. medical domain, legal domain, …). In fact, the common practice when building specialized versions of BERT (e.g. BioBERT [4], BlueBERT [5] and some SciBERT [6] models) is to re-train the original model on a set of specialized texts. As a result, most SOTA specialized models keep the original general-domain WordPiece vocabulary which is not suited for specialized domain applications.\nThe table below shows the difference between the original general-domain vocabulary and a medical WordPiece vocabulary that was built on medical corpora: MIMIC [7] and PMC OA [8].\nTokenization of domain-specific terms using wordpiece vocabularies of different domains (Source: [2])\nWe can clearly see that BERT’s vocabulary is not very well suited for specialized terms (e.g. “choledocholithiasis” is split into [cho, led, och, oli, thi, asi, s]). The medical wordpiece works better however it has its limits as well (e.g. “borborygmi” into [bor, bor, yg, mi]). Therefore, in order to avoid any biases that may come from using the wrong WordPiece vocabulary, and in an effort to got back to conceptually simpler models, a variant of BERT was proposed: CharacterBERT.\nHow is CharacterBERT tested against BERT? BERT and CharacterBERT are compared in a classic scenario where a general model is first pre-trained before serving as an initialisation for the pre-training of a specialized version.\n Note: we focus here on the English language and the medical domain.\n Pre-training corpora (Source: [2])\nTo be as fair as possible, both BERT and CharacterBERT are pre-trained from scratch in exactly the same conditions. Then, each pre-trained model is evaluated on multiple medical tasks. Let’s take an example.\nEvaluation tasks (Source: [2])\ni2b2/VA 2010 [9] is a competition that consists in multiple tasks including the clinical concept detection task which was used to evaluate our models. The goal is to detect three types of clinical concepts: Problem, Treatment and Test. An example is given in the far left section of the figure above.\nAs usual, we evaluate our models by first training on the training set. At each iteration, the model is tested on a separate validation set, allowing us to save the best iteration. Finally, after going through all the iterations, a score (here a strict F1 score) is computed on the test set using the model from the best iteration. This whole procedure is then repeated 9 more times using different random seeds, which allows us to account for some of the variance and report final model performances as: mean ± std.\n Note: more details are available in the paper [2]\n What are the results? Evaluation results (Source: [2])\nIn most cases, CharacterBERT outperformed its BERT counterpart.\n Note: The only exception is the ClinicalSTS task where the medical CharacterBERT got (on average) a lower score than the BERT version. This may be due to the task dataset being small (1000 examples vs. 30,000 on average for other tasks) and should be investigated.\n Bonus: Robustness to Noise Besides pure performance, another interesting aspect is whether the models are robust to noisy inputs. In fact, we evaluated BERT and CharacterBERT on noisy versions of the MedNLI task [10] where (put simply) the goal is to say whether two medical sentences are in contradiction with each other. Here, a noise level of X% means that each character in the text if either replaced or swapped with a X% probability. The results are displayed on the figure below.\nBERT and CharacterBERT fine-tuned on noisy (misspelled) versions of MEDNLI (Source: [2])\nAs you can see, the medical CharacterBERT model seems to be more robust than medical BERT: the initial gap between the two models of ~1% accuracy grows to ~3% when adding noise to all splits, and ~5% when surprising the models with noise only in the test set.\nWhat about the downsides of CharacterBERT? The main downside of CharacterBERT is its slower pre-training speed. This is due to:\n the CharacterCNN module that is slower to train; but mainly because the model is working at the token-level:\nit updates a large token vocabulary at each pre-training iteration.   Note: However, CharacterBERT is just as fast as BERT during inference (actually, it is even a bit faster) and pre-trained models are available so you can skip the pre-training step altogether 😊!\n  Conclusion All in all, CharacterBERT is a simple variant of BERT that replaces the WordPiece system with a CharacterCNN (just like ELMo before that). Evaluation results on multiple medical tasks show that this change is beneficial: improved performance \u0026 improved robustness to misspellings. Hopefully, this model will motivate more research towards word-level open-vocabulary transformer-based language models (e.g. applying the same idea to ALBERT [11], ERNIE [12]).\n Original paper:\nhttps://arxiv.org/abs/2010.10392\n  Code \u0026 pre-trained models: https://github.com/helboukkouri/character-bert\n  Work done by:\nHicham El Boukkouri (myself), Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum and Junichi Tsujii\n References  [1] Peters, Matthew E., et al. “Deep contextualized word representations.” arXiv preprint arXiv:1802.05365 (2018). [2] El Boukkouri, Hicham, et al. “CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters.” arXiv preprint arXiv:2010.10392 (2020). [3] Srivastava, Rupesh Kumar, Klaus Greff, and Jürgen Schmidhuber. “Highway networks.” arXiv preprint arXiv:1505.00387 (2015). [4] Lee, Jinhyuk, et al. “BioBERT: a pre-trained biomedical language representation model for biomedical text mining.” Bioinformatics 36.4 (2020): 1234–1240. [5] Peng, Yifan, Shankai Yan, and Zhiyong Lu. “Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets.” arXiv preprint arXiv:1906.05474 (2019). [6] Beltagy, Iz, Kyle Lo, and Arman Cohan. “SciBERT: A pretrained language model for scientific text.” arXiv preprint arXiv:1903.10676 (2019). [7] Johnson, Alistair, et al. “MIMIC-III Clinical Database” (version 1.4). PhysioNet (2016), https://doi.org/10.13026/C2XW26. [8] PMC OA corpus: https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ [9] Uzuner, Özlem, et al. “2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text.” Journal of the American Medical Informatics Association 18.5 (2011): 552–556. [10] Shivade, Chaitanya. “MedNLI - A Natural Language Inference Dataset For The Clinical Domain” (version 1.0.0). PhysioNet (2019), https://doi.org/10.13026/C2RS98. [11] Lan, Zhenzhong, et al. “ALBERT: A lite BERT for self-supervised learning of language representations.” arXiv preprint arXiv:1909.11942 (2019). [12] Sun, Yu, et al. “ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding.” AAAI. 2020.   ","wordCount":"1370","inLanguage":"en","datePublished":"2020-11-09T20:59:15+01:00","dateModified":"2020-11-09T20:59:15+01:00","author":{"@type":"Person","name":"Hicham EL BOUKKOURI"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://helboukkouri.github.io/posts/character-bert/"},"publisher":{"@type":"Organization","name":"Hicham' Blog","logo":{"@type":"ImageObject","url":"https://helboukkouri.github.io/images/favicon.ico"}}}</script></head><body class=single id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><noscript><style type=text/css>.theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://helboukkouri.github.io/ accesskey=h>Home</a>
<span class=logo-switches><span class=theme-toggle><a id=theme-toggle accesskey=t><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></span></span></div><ul class=menu id=menu onscroll=menu_on_scroll()><li><a href=https://helboukkouri.github.io/posts/><span>Blog Posts</span></a></li><li><a href=https://helboukkouri.github.io/embedding-visualization/><span>Embedding Universe</span></a></li></ul></nav><hr></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</h1><div class=post-meta>November 9, 2020&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Hicham EL BOUKKOURI</div></header><div class=toc><details open><summary><div class=details accesskey=c>Table of Contents</div></summary><blockquote><ul><li><a href=#what-is-characterbert-and-how-is-it-different-from-bert aria-label="What is CharacterBERT and how is it different from BERT ?">What is CharacterBERT and how is it different from BERT ?</a></li><li><a href=#why-characterbert-instead-of-bert aria-label="Why CharacterBERT instead of BERT?">Why CharacterBERT instead of BERT?</a></li><li><a href=#how-is-characterbert-tested-against-bert aria-label="How is CharacterBERT tested against BERT?">How is CharacterBERT tested against BERT?</a></li><li><a href=#what-are-theresults aria-label="What are the results?">What are the results?</a></li><li><a href=#bonus-robustness-tonoise aria-label="Bonus: Robustness to Noise">Bonus: Robustness to Noise</a></li><li><a href=#what-about-the-downsides-of-characterbert aria-label="What about the downsides of CharacterBERT?">What about the downsides of CharacterBERT?</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></blockquote></details></div><div class=post-content><hr><h2 id=what-is-characterbert-and-how-is-it-different-from-bert>What is CharacterBERT and how is it different from BERT ?</h2><p>CharacterBERT is a variant of BERT that tries to go back to the simpler days where <strong>models produced single embeddings for single words</strong> (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a <strong>CharacterCNN module just like the one that was used in ELMo</strong> <em><a href=#reference.1>[1]</a></em>.</p><p>The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.</p><p><img src=/images/character-bert/characterbert.png alt="alt text" title="BERT vs. CharacterBERT"></p><center><small>Context-independent token representations in BERT vs. in CharacterBERT (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>Let&rsquo;s imagine that the word &ldquo;<strong>Apple</strong>&rdquo; is an unknown word (i.e. it does not appear in BERT&rsquo;s WordPiece vocabulary), then BERT <strong>splits it into known WordPieces</strong>: [<strong>Ap</strong>, <strong>##ple</strong>], where <strong>##</strong> are used to designate WordPieces that are not at the beginning of a word. Then, each subword unit is <strong>embedded using a WordPiece embedding matrix</strong>, producing <strong>two output vectors</strong>.</p><p>On the other hand, <strong>CharacterBERT does not have a WordPiece vocabulary and can handle any input token as long</strong> as it is not unreasonably long (i.e. under 50 characters). Instead of splitting &ldquo;<strong>Apple</strong>&rdquo;, CharacterBERT reads it as a <strong>sequence of characters</strong>: [<strong>A</strong>, <strong>p</strong>, <strong>p</strong>, <strong>l</strong>, <strong>e</strong>]. Each character is then <strong>represented using a character embedding matrix</strong>, producing a sequence of <strong>character embeddings</strong>. This sequence is then <strong>fed to multiple CNNs</strong>, each responsible for scanning the sequence <strong>n-characters at a time</strong>, with n =[1..7]. All CNN outputs are <strong>aggregated into a single vector</strong> that is then <strong>projected down</strong> to the desired dimension using Highway Layers <em><a href=#reference.3>[3]</a></em>. This final projection is the <strong>context-independent representation</strong> of the word &ldquo;Apple&rdquo;, which will be combined with position and segment embeddings before being fed to multiple Transformer Layers as in BERT.</p><h2 id=why-characterbert-instead-of-bert>Why CharacterBERT instead of BERT?</h2><p>CharacterBERT acts almost as a <strong>drop in replacement for BERT</strong> that</p><ul><li>Produces a single embedding for any input token</li><li>Does not rely on a WordPiece vocabulary</li></ul><p>The first point is clearly desirable as working with single embeddings is <strong>far more convenient</strong> than having a variable number of WordPiece vectors for each token. As for the second point, it is particularly relevant when working in <strong>specialized domains</strong> (e.g. medical domain, legal domain, …). In fact, the common practice when building <strong>specialized versions of BERT</strong> (e.g. BioBERT <em><a href=#reference.4>[4]</a></em>, BlueBERT <em><a href=#reference.5>[5]</a></em> and some SciBERT <em><a href=#reference.6>[6]</a></em> models) is to <strong>re-train</strong> the original model on a set of specialized texts. As a result, most SOTA specialized models <strong>keep the original general-domain WordPiece vocabulary</strong> which is not suited for specialized domain applications.</p><p>The table below shows the difference between the <strong>original general-domain vocabulary</strong> and a <strong>medical WordPiece vocabulary</strong> that was built on medical corpora: MIMIC <em><a href=#reference.7>[7]</a></em> and PMC OA <em><a href=#reference.8>[8]</a></em>.</p><p><img src=/images/character-bert/medical-terms.png alt="alt text" title="Medical term WordPiece tokenization"></p><center><small>Tokenization of domain-specific terms using wordpiece vocabularies of different domains (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>We can clearly see that BERT&rsquo;s vocabulary is not very well suited for specialized terms (e.g. &ldquo;choledocholithiasis&rdquo; is split into [cho, led, och, oli, thi, asi, s]). The medical wordpiece works better however it has its limits as well (e.g. &ldquo;borborygmi&rdquo; into [bor, bor, yg, mi]). Therefore, in order to <strong>avoid any biases</strong> that may come from using the wrong WordPiece vocabulary, and in an effort to got back to <strong>conceptually simpler models</strong>, a variant of BERT was proposed: CharacterBERT.</p><h2 id=how-is-characterbert-tested-against-bert>How is CharacterBERT tested against BERT?</h2><p>BERT and CharacterBERT are compared in a classic scenario where a general model is first pre-trained before serving as an initialisation for the pre-training of a specialized version.</p><blockquote><p><strong>Note</strong>: <em>we focus here on the English language and the medical domain.</em></p></blockquote><p><img src=/images/character-bert/pretraining-corpora.png alt="alt text" title="Pre-training corpora"></p><center><small>Pre-training corpora (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>To be <strong>as fair as possible</strong>, both BERT and CharacterBERT are pre-trained from scratch in <strong>exactly the same conditions</strong>. Then, each pre-trained model is evaluated on multiple medical tasks. Let&rsquo;s take an example.</p><p><img src=/images/character-bert/evaluation-tasks.png alt="alt text" title="Evaluation tasks"></p><center><small>Evaluation tasks (Source: <a href=#reference.2>[2]</a>)</small></center><br><p><strong>i2b2/VA 2010</strong> [9] is a competition that consists in multiple tasks including the <strong>clinical concept detection task</strong> which was used to evaluate our models. The goal is to detect three types of clinical concepts: <strong>Problem</strong>, <strong>Treatment</strong> and <strong>Test</strong>. An example is given in the far left section of the figure above.</p><p>As usual, we evaluate our models by first training on the training set. At each iteration, the model is tested on a separate validation set, allowing us to save the best iteration. Finally, after going through all the iterations, a score (here a strict F1 score) is computed on the test set using the model from the best iteration. This whole procedure is then repeated 9 more times using different random seeds, which allows us to account for some of the variance and report final model performances as: <strong>mean ± std</strong>.</p><blockquote><p><strong>Note</strong>: <em>more details are available in the paper <a href=#reference.2>[2]</a></em></p></blockquote><h2 id=what-are-theresults>What are the results?</h2><p><img src=/images/character-bert/evaluation-results.png alt="alt text" title="Evaluation results"></p><center><small>Evaluation results (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>In most cases, <strong>CharacterBERT outperformed its BERT counterpart</strong>.</p><blockquote><p><strong>Note</strong>: <em>The only exception is the ClinicalSTS task where the medical CharacterBERT got (on average) a lower score than the BERT version. This may be due to the task dataset being small (1000 examples vs. 30,000 on average for other tasks) and should be investigated.</em></p></blockquote><h2 id=bonus-robustness-tonoise>Bonus: Robustness to Noise</h2><p>Besides pure performance, another interesting aspect is <strong>whether the models are robust to noisy inputs</strong>. In fact, we evaluated BERT and CharacterBERT on noisy versions of the MedNLI task <em><a href=#reference.10>[10]</a></em> where (put simply) the goal is to say whether two medical sentences are in contradiction with each other. Here, a noise level of X% means that each character in the text if either replaced or swapped with a X% probability. The results are displayed on the figure below.</p><p><img src=/images/character-bert/noise-robustness.png alt="alt text" title="Robustness to noise"></p><center><small>BERT and CharacterBERT fine-tuned on noisy (misspelled) versions of MEDNLI (Source: <a href=#reference.2>[2]</a>)</small></center><br><p>As you can see, the medical CharacterBERT model seems to be <strong>more robust than medical BERT</strong>: the initial gap between the two models of ~1% accuracy grows to ~<strong>3%</strong> when adding noise to all splits, and ~<strong>5%</strong> when surprising the models with noise only in the test set.</p><h2 id=what-about-the-downsides-of-characterbert>What about the downsides of CharacterBERT?</h2><p>The main downside of CharacterBERT is its <strong>slower pre-training speed</strong>. This is due to:</p><ul><li>the CharacterCNN module that is <strong>slower to train</strong>;</li><li>but mainly because the model is working at the <strong>token-level</strong>:<br>it updates a <strong>large token vocabulary</strong> at each pre-training iteration.</li></ul><blockquote><p><strong>Note</strong>: <em>However, CharacterBERT is just as fast as BERT <strong>during inference</strong> (actually, it is even a bit faster) and <strong>pre-trained models are available</strong> so you can skip the pre-training step altogether 😊!</em></p></blockquote><hr><h2 id=conclusion>Conclusion</h2><p>All in all, CharacterBERT is a <strong>simple variant of BERT</strong> that replaces the WordPiece system with a CharacterCNN (just like ELMo before that). Evaluation results on multiple medical tasks show that this change is beneficial: <strong>improved performance</strong> & <strong>improved robustness to misspellings</strong>. Hopefully, this model will motivate more research towards word-level open-vocabulary transformer-based language models (e.g. applying the same idea to ALBERT <em><a href=#reference.11>[11]</a></em>, ERNIE <em><a href=#reference.12>[12]</a></em>).</p><blockquote><p><strong>Original paper</strong>:<br><a href=https://arxiv.org/abs/2010.10392>https://arxiv.org/abs/2010.10392</a></p></blockquote><blockquote><p><strong>Code & pre-trained models</strong>:<br><a href=https://github.com/helboukkouri/character-bert>https://github.com/helboukkouri/character-bert</a></p></blockquote><blockquote><p><strong>Work done by</strong>:<br>Hicham El Boukkouri (myself), <a href="https://scholar.google.com/citations?user=-mCQhtIAAAAJ&hl=en">Olivier Ferret</a>, <a href="https://scholar.google.fr/citations?user=l7XLFhEAAAAJ&hl=en">Thomas Lavergne</a>, <a href="https://scholar.google.com/citations?user=OODRveoAAAAJ&hl=en">Hiroshi Noji</a>, <a href="https://scholar.google.fr/citations?user=0LjUNAsAAAAJ&hl=en">Pierre Zweigenbaum</a> and <a href="https://scholar.google.ca/citations?user=h3aNnAIAAAAJ&hl=en">Junichi Tsujii</a></p></blockquote><h2 id=references>References</h2><ul><li><a id=reference.1></a>[1] Peters, Matthew E., et al. &ldquo;Deep contextualized word representations.&rdquo; arXiv preprint arXiv:1802.05365 (2018).</li><li><a id=reference.2></a>[2] El Boukkouri, Hicham, et al. &ldquo;CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters.&rdquo; arXiv preprint arXiv:2010.10392 (2020).</li><li><a id=reference.3></a>[3] Srivastava, Rupesh Kumar, Klaus Greff, and Jürgen Schmidhuber. &ldquo;Highway networks.&rdquo; arXiv preprint arXiv:1505.00387 (2015).</li><li><a id=reference.4></a>[4] Lee, Jinhyuk, et al. &ldquo;BioBERT: a pre-trained biomedical language representation model for biomedical text mining.&rdquo; Bioinformatics 36.4 (2020): 1234–1240.</li><li><a id=reference.5></a>[5] Peng, Yifan, Shankai Yan, and Zhiyong Lu. &ldquo;Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets.&rdquo; arXiv preprint arXiv:1906.05474 (2019).</li><li><a id=reference.6></a>[6] Beltagy, Iz, Kyle Lo, and Arman Cohan. &ldquo;SciBERT: A pretrained language model for scientific text.&rdquo; arXiv preprint arXiv:1903.10676 (2019).</li><li><a id=reference.7></a>[7] Johnson, Alistair, et al. &ldquo;MIMIC-III Clinical Database&rdquo; (version 1.4). PhysioNet (2016), <a href=https://doi.org/10.13026/C2XW26>https://doi.org/10.13026/C2XW26</a>.</li><li><a id=reference.8></a>[8] PMC OA corpus: <a href=https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/>https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/</a></li><li><a id=reference.9></a>[9] Uzuner, Özlem, et al. &ldquo;2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text.&rdquo; Journal of the American Medical Informatics Association 18.5 (2011): 552–556.</li><li><a id=reference.10></a>[10] Shivade, Chaitanya. &ldquo;MedNLI - A Natural Language Inference Dataset For The Clinical Domain&rdquo; (version 1.0.0). PhysioNet (2019), <a href=https://doi.org/10.13026/C2RS98>https://doi.org/10.13026/C2RS98</a>.</li><li><a id=reference.11></a>[11] Lan, Zhenzhong, et al. &ldquo;ALBERT: A lite BERT for self-supervised learning of language representations.&rdquo; arXiv preprint arXiv:1909.11942 (2019).</li><li><a id=reference.12></a>[12] Sun, Yu, et al. &ldquo;ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding.&rdquo; AAAI. 2020.</li></ul><hr></div><footer class=post-footer><div class=pre-share-buttons><p>Found this post interesting? Don't hesitate to share it on your favorite medium:</p><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on twitter" href="https://twitter.com/intent/tweet/?text=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&url=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f&title=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&summary=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters&source=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f&title=CharacterBERT%3a%20Reconciling%20ELMo%20and%20BERT%20for%20Word-Level%20Open-Vocabulary%20Representations%20From%20Characters"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhelboukkouri.github.io%2fposts%2fcharacter-bert%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></div></div></footer></article></main><footer class=footer><span>&copy; 2020 <a href=https://helboukkouri.github.io/>Hicham' Blog</a></span>
<span>&#183;</span>
<span>Made with <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> using <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top" accesskey=g><button class=top-link id=top-link type=button><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg></button></a>
<script src=https://helboukkouri.github.io/assets/js/highlight.min.e7afc2928c0925d65c4732dfebe147014d91299a98e819e4b42f25c4fa68e91c.js integrity="sha256-56/CkowJJdZcRzLf6+FHAU2RKZqY6BnktC8lxPpo6Rw="></script><script>hljs.initHighlightingOnLoad();</script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);document.querySelector(`[id='${id}']`).scrollIntoView({behavior:"smooth"});});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>