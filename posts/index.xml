<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Hicham&#39; Blog</title>
    <link>https://helboukkouri.github.io/posts/</link>
    <description>Recent content in Posts on Hicham&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 09 Nov 2020 20:59:15 +0100</lastBuildDate><atom:link href="https://helboukkouri.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</title>
      <link>https://helboukkouri.github.io/posts/character-bert/</link>
      <pubDate>Mon, 09 Nov 2020 20:59:15 +0100</pubDate>
      
      <guid>https://helboukkouri.github.io/posts/character-bert/</guid>
      <description>What is CharacterBERT and how is it different from BERTÂ ? CharacterBERT is a variant of BERT that tries to go back to the simpler days where models produced single embeddings for single words (or rather, tokens). In practice, the only difference is that instead of relying on WordPieces, CharacterBERT uses a CharacterCNN module just like the one that was used in ELMo [1].
The next figure show the inner mechanics of the CharacterCNN and compares it to the original WordPiece system in BERT.</description>
    </item>
    
  </channel>
</rss>
